---
title: SLAM 学习学期总结
date: 2019-06-15 12:00:00
categories:
- SLAM

mathjax: true

---

## 0. 高级实训实验报告

### 实验内容

本次实验使用安卓手机摄像头拍摄视频数据模仿[KITTI数据集](http://www.cvlibs.net/datasets/kitti/eval_object.php)，测试ORB_SLAM2系统([github](https://github.com/raulmur/ORB_SLAM2))单目用例，并添加地图保存及加载功能。

**实验用具**

计算机一台，华为麦芒5手机一台，6*8棋盘一张。

### 实验步骤

1. 相机标定：使用棋盘法对手机摄像头进行标定得到相机内参和畸变系数，填入yaml文件。

2. 准备数据集：用手机拍摄多个场景视频并处理成图片集和时间戳文件。
3. 运行ORB_SLAM2单目系统进行定位和建图。
4. 添加地图保存及加载功能。

### 实验原理

- 张氏棋盘标定法 

- ORBSLAM2地图元素分析

  为了能够将三个线程状态和地图复原出来，需要保存地图`Map`中各个元素以及元素之间的关系，包括3D地图点，关键帧、BoW向量、共视图（covisibility graphs）、生成树等等。在tracking过程的局部地图跟踪中需要用到3D地图点、covisibility等，reference frame处理需要用到关键帧的BoW向量，重定位需要用到BoW向量、3D点等。

  同时，将**关键帧**类拆开会得到这一帧包含的2D特征点、描述子、相机位姿、关键帧ID，3D地图点（由特征点进行三角化得到）。**地图点**则需要ID和世界坐标。由于在关键帧中包含了特征点和描述子，我们不保存BoW向量而是在加载了关键帧后用特征描述子重新计算。

### 实验过程

1. **棋盘标定**

- 打印一张棋盘贴在纸板上，用手机从不同角度拍摄一段视频并转换为图片序列。

  - 原始视频:[123.mp4](.\assets\123.mp4)；
  - 图像示例: originframe000.png

  ![originframe000](.\assets\originframe000.png)

- 编写python脚本进行标定

  - 使用opencv下的几个主要函

    > 1. 将彩色图转换为灰度图
  > 2. 数对每一帧处理
  >    1. 用cv2.findChessboardCorners函数得到棋盘内角点坐标
  >    2. 用cv2.cornerSubPix获得亚像素精度下的内角点坐标
  >    3. 保存图像平面的2D坐标和世界空间的3D坐标
  >    4. 用cv2.drawChessboardCorners将内角点绘制在原图上
  > 3. 用cv2.calibrateCamera得到内参矩阵、畸变系数

  - 示例:img000.png

    ![](.\assets\img000.png)

  - 实验截图

    ![](.\assets\camera.png)

- 拷贝 `ORB_SLAM2/Examples/Monocular/KITTI00-02.yaml`得到`test3.yaml`并将参数填到相应位置:

  ![yaml](.\assets\yaml.png)



2. **准备数据集**

- 在中大图书馆内使用手机拍摄一段视频，每秒钟提取两帧图片并根据时间顺序编号，模仿KITTI数据集将时间戳文件`time.txt`和图片序列文件夹`image_0`放在同一文件夹`preprocessing/00`中。

  - 原始视频:[testVideo/test1.mp4](.\assets\test1.mp4)，共生成**318**张测试图像。

  - 图片示例:`000000.png`

    ![000000.png](.\assets\000000.png)

  - 时间戳文件`time.txt`：

    ```
    0.000000000000000000e+00
    1.332939504246836437e-01
    1.999409256370254795e-01
    2.665879008493672875e-01
    3.332348760617090955e-01
    3.998818512740509035e-01
    4.665288264863927115e-01
    5.331758016987345750e-01
    5.998227769110764385e-01
    6.664697521234183020e-01
    7.331167273357601655e-01
    7.997637025481020290e-01
    8.664106777604438925e-01
    9.330576529727857560e-01
    9.997046281851276195e-01
    .....
    ```

    

3. **运行**

- 此前已经编译好ORB_SLAM2的源码了。进入`ORB_SLAM2`并 用以下命令运行`mono_kitti`用例（用例+字典路径+配置文件路径+数据集路径）。

  ```
  ./Examples/Monocular/mono_kitti ./Vocabulary/ORBvoc.txt ./Examples/test3.yaml ~/Desktop/src/preprocessing/00
  ```

- [**运行录屏**](./asserts/test1-1.mp4)

  ![截屏](.\assets\test1-1.png)

4. **地图保存**

- 先存地图点再存关键帧。具体保存元素的顺序：

  > - 3D地图点个数
  >
  > - 顺序存放地图点：
  >   - 地图点ID
  >   - x，y，z坐标
  > - 关键帧个数
  > - 顺序存放关键帧：
  >   - 关键帧ID
  >   - 关键帧时间戳
  >   - 关键帧的相机位姿：
  >     - 四元数（4个float）
  >     - 平移向量（3个float）
  >   - 顺序存放关键点（cv::KeyPoint）
  >     - x，y - 坐标
  >     - size - 邻域直径
  >     - angle - 方向
  >     - response - 响应强度
  >     - octacv - 从哪一层金字塔得到的此关键点
  >     - 描述子
  >     - 与该关键点对应的地图点的ID
  > - 顺序存放covisibility graph/spanningtree (按关键帧顺序)
  >   - 关键帧的父节点ID
  >   - 相邻(connected)帧的ID和权重weight

- 步骤
  1. 修改`Map.h`头文件及其实现:

  ```c++
  public:
      // add save map function
      void Save(const string &filename);
  protected:
      //save map point / key frame
      void SaveMapPoint(ofstream &f,MapPoint* mp);
      void SaveKeyFrame(ofstream &f,KeyFrame* kf);
  ```

  2. 将旋转矩阵通过四元素+平移向量的方式存储，在`Converter`类中添加一个矩阵和四元素相互转换的函数:

  ```c++
  cv::Mat Converter::toCvMat(const std::vector<float>& v)
  ```

  3. 修改`System.h`头文件及其实现

  ```c++
  void SaveMap(const string &filename); 
  ```

  4. 在`mono_kitti.cc`中家地图存储代码进行测试（Shutdown后面）

  ```c++
  SLAM.SaveMap("/home/yuki/allForSLAM/ORB_SLAM2/Examples/Monocular/map.bin");
  ```

  5. 重新编译运行

  > chmod +x ./build.sh
  >
  > ./build.sh

- 输出（打印出了前10个地图点）s

  ![](.\assets\mapSaving.png)

5. **地图加载**

- 地图保存的逆过程

- 步骤
  1. 修改`Map.h`头文件及其实现:

      ```c++
      void Load(const string &filename,SystemSetting* mySystemSetting);
      MapPoint* LoadMapPoint(ifstream &f);
      KeyFrame* LoadKeyFrame(ifstream &f,SystemSetting* mySystemSetting);
      ```

  2. 在`MapPoint`类中添加构造函数使之能够从仅有position和当前Map的情况中恢复出原来的地图点:

      ```c++
      MapPoint(const cv::Mat &Pos,Map* pMap);
      //另一添加:设置参考帧
      KeyFrame* SetReferenceKeyFrame(KeyFrame* RFKF){ return mpRefKF = RFKF; };
      ```

  3. 在`KeyFrame`类中添加构造函数:

     ```c++
     KeyFrame(InitKeyFrame &initkf,Map* pMap,KeyFrameDatabase* pKFDB,vector<MapPoint*>& vpMapPoints);
     ```

  4. 修改`System.h`头文件及其实现

      ```c++
      void LoadMap(const string &filename);
      //添加数据成员保存参数文件的读取结果
      std::string mySettingFile;
      ```

    5. 添加**SystemSetting**类和**InitKeyFrame**类，用于读取参数文件和关键帧初始化

  6. 在`mono_kitti.cc`中家地图存储代码进行测试（Shutdown后面）
  ```c++
    SLAM.LoadMap("/home/ORB_SLAM2/Examples/Stereo/map.bin");
  ```

    7. 在`CMakeLists.txt `文件中`add_library` 中加入 `src/InitkeyFrame.cc` `src/SystemSetting.cc`，重新编译运行 
  
    ```c++
  //mono_kitti.cc :: main
    ORB_SLAM2::System SLAM2(argv[1],argv[2],ORB_SLAM2::System::MONOCULAR,true);

    // test map loading
    SLAM2.LoadMap("/home/yuki/allForSLAM/ORB_SLAM2/Examples/Monocular/map.bin");
      
    cerr << "press any key to quit" << endl;
    getchar();
      
    SLAM2.Shutdown();
    ```


  ​    


- 输出

  ![](.\assets\mapLoading-output.png)

  ![](.\assets\mapLoading.png)

  

**补充**

1. 关于方案的设计

   - 时间有限选择了先完成最保守的方案；
   - ROS+笔记本摄像头（或者usb摄像头）实时拍摄并处理。我这台笔记本的摄像头是可以用的，但是由于我是在笔电上远程操作宿舍的主机跑slam，现在还没有能够找到将笔电摄像头和远程主机结合的方法（拿什么做实时视频流传输）
   - ROS+手机摄像头实时：手机和处理器需要在同一局域网下
   - windows+Ros+笔记本摄像头：目前硬件可以实现的方案
   - 参考
        - [ROS下调用笔记本摄像头](https://blog.csdn.net/qq_39989653/article/details/80393912)
        - [ROS下的相机标定和ORB-SLAM运行](<https://www.cnblogs.com/shang-slam/p/6733322.html>)
        - [在ROS环境下对笔记本自带的摄像头（单目）进行标定](https://blog.csdn.net/zhuquan945/article/details/73166087)

2. 关于数据集的采集：

   - 室外/半室外的场景几乎都没有办法通过初始化环节，日光对与关键点的提取干扰很强，原因待考究，但是之前统一选地拍摄时候，选取的大部分场景的数据都不能用。

   - 会在答辩前出一个含回环检测测试的数据集的v2。

3. 一些坑

   - ROS对python3的支持很不好。安装ROS时要切换成python2。由于添加了系统一些路径，再切换回python3不能import cv2模块。[参考](https://blog.csdn.net/qq_34544129/article/details/81946494)。可以直接注释掉了，使用ROS的时候再source回来并转回python2

     ```
     update-alternatives --config python
     ```







## 1. SLAM技术综述

- 定位，建图，导航，探索
- 假设已经有地图：定位和导航（路径规划），不需要探索。
  - 应用场景：田地

- 在移动机器中，考虑如何建立地图，同时进行定位和建图。
  - 根据传感器的不同分为激光SLAM和视觉SLAM，后者为本次实训课题。

### V-SLAM通用框架

![1562745779914](.\assets\1562745779914.png)

- 主流相机

  *根据是否提供深度信息的不同，视觉里程计算法不同*

  - 单目相机：单帧照片估计；一开始的问题是通过照片和照片之间的比对确定相机的运功（2d到2d），通过分解essential矩阵/fundamental矩阵/homographic矩阵 估计，存在尺度不确定问题（主要原因是如果将地图和轨迹同时放大和缩小结果时不变的）
  - 双目相机：有深度图给出深度信息难度降低，没有尺度不确定问题：通过左右眼视差估计距离，特别是要计算每一个像素的距离的时候计算机的计算量非常大
  - 深度RGBD相机：有深度图给出深度信息，没有尺度不确定问题：通过投射光并测量返回光的时间和pattern来物理估计

- 前端
  - 传感器数据进入系统进行前端数据处理：
    - 粗略的位置和地图表示
    - 根据传感器不同有不同的算法；

  - 分类
      - 激光SLAM
          - 激光传感器：SLAM matching （激光点匹配）*sebastian thrun*
          - 早期是激光定位问题（概率机器人学），大多数是rose提供的算法，算法实现本身研究少
          - 2000往后一直是建立高精度地图而不是建立更像人类认知的拓扑地图
          - 方式和产业已经比较成熟，做开题文章比较难，但是在室内做类似动物的导航的避障探索（可以做），现有方法是全局和局部耦合，如何利用激光数据进行训练，使得可以在多障地形中导航
          - 如何在嵌入式arm方面（计算资源少）去做？可以选择精度比较高的传感器；计算资源消耗的多少与particle（粒子数）相关，建图（15~25）比，激光越精确，用到的粒子数更少；因为往往需要很多粒子数的做比较精确的估计，减少出现回环错误的概率；
          - 现有的扫地机器人的地图构建精度不高
          - gmapping：占据栅格图所用的建图方法
      - 视觉SLAM（相机）   前端：==视觉里程计== （CV+测量几何）
          - 主要区别：根据图像估计相机运动
              1. 相机运动一般是3d运动，需要精确描述相机在三维空间的位置，需要大量矩阵知识（旋转矩阵）
              2. 如何根据图像之间的差异将运动估计出来：纷争特征点法和直接法两类
          - 视觉里程计的两种方法：
              - 基于==特征点==（主流）：orb-slam 
                - 通过特征提取和特征匹配抓取一组匹配点并得到匹配点的空间关系来估计相机运动（不管用哪一种相机的数据）
                  - 缺点：特征点在空间经常过于集中（纹理信息丰富，像素变化明显区域），在稀疏；在梯度变化不明晰的地方容易特征确实（白墙等）；计算量也非常大，仅能在pc端正常运行；
              - 基于==直接法==（近年）：以svo和lsd为代表，从光流发展而来；
                - 根据图像像素变化估计：假设同一个像素在不同视角下灰度是一样的，主要是构建一个最小化光度误差的优化问题并求解
                - 分类（使用像素的多少）：稠密（整幅图像），半稠密（使用像素梯度明显的地方），稀疏（关键点：像素变化明显的角点或者边缘）
                - 优点：节省特征描述的计算，弥补特征丢失的问题
                - 代表性工作：dbo lsd dso svo，目前稀疏直接法速度最快：svo2.0（代码未开源）论文中提出能够在pa（？）上达到200赫兹的速度甚至更快；dso也可以达到120左右特征点法不能做到
        - 视觉里程计使用的是相邻帧图像的运动信息，累计误差不可避免。希望得到一致的轨迹就需要后端优化和回环检测

- 后端
  - EKF（已抛弃）
  - 非线性优化

- 回环检测
  - 测量误差默认高斯分布，构建一个最小二乘问题通过非线性优化求解
  - 在视觉SLAM中更有效（因为图像信息位置比对信息更丰富）
  - 词袋？bag of word 视觉回环检测最主流，在前端中也可用于加速匹配

- 空间认知
  - 输入1为自身的运动估计（rotation，self-motion等），输入2为visial input（眼睛捕获环境图像）
  - 传感器组合

## 2. ORB-SLAM 

### 1. 论文摘要

*ORB-SLAM: a Versatile and Accurate Monocular SLAM System* 最初于2015.10发表在 IEEE Transactions on Robotics上，介绍了基于特征点法的经典系统 ORB-SLAM。



src: [ORB-SLAM: a Versatile and Accurate Monocular SLAM System](https://ieeexplore.ieee.org/document/7219438)



重要概念

- Covisibility Graph
- Essential Graph

![1562748203941](.\assets\1562748203941.png)

**简介**

一个实时SLAM算法的BA应该有如下要求

1. 在候选图像帧子集（关键帧集）中，匹配观测的场景特征（地图云点）。
2. 因为计算复杂度随着关键帧的数量的增长而增长，需要筛选关键帧，避免冗余；
3. 关键帧和云点的的强网络结构能够有较为精确的结果，关键帧在观察点的时候能够有显著的视差（parallax）和足够的回环匹配（loop closure matches）
4. 对于关键帧和云点文职的初始估计，采用非线性优化
5. （探索）局部地图（优化的关键是获得良好的稳定性）
6. 实时执行快速全局优化（如pose graph）闭环回路



单目ORB-SLAM系统的主要技术及组件

1. 对所有任务（跟踪，建图，重定位，回环检测）使用相同的特征：ORB特征，具有旋转不变性，在无GPU下得到较好的实时性能。
2. 实时户外环境中的操作。由于使用了**covisibility graph**（视图内容关联？），跟踪和建图都关注一个局部covisible（视图关联）区域，可以独立于全局地图进行操作。
3. 基于位姿图优化的实时loop closing：*Essential Graph*。由生成树构建，生成树由系统、闭环控制链和来自covisibility graph的强边缘维护（It is built from a spanning tree maintained by the system, loop closure links, and strong edges from the covisibility graph）
4. 实时相机重定位具有旋转不变性，视角不变性和光度不变性，使得系统能够从跟踪失败中恢复，也能增强地图重用。
5. 基于不同的模型选择，会有不同的初始化过程，能够创建不同二维和非二维场景的初始地图，也是自动且具有良好鲁棒性的。
6. 用于地图云点和关键帧选择的*survival of the fittest*方法，“大量生成但挑选严格”



**相关工作**

1. Place Recognition：论文13比较了几种基于图像处理技术的位置识别方法。其中FAST特征检测和BRIEF描述子产生的二进制词袋可以通过DBoW2获得，与传统的SURF和SIFT相比计算时间小一个数量级。我们改进了DBoW2提供的词袋得到的ORB特征，具有旋转不变和尺度不变性。
2. Map Initialization
3. Monocular Sinultaneous Localization and Mapping



**系统架构**

1. 特征选择feature choice ：用于建图跟踪和地点识别（帧率重定位和回环检测）的特征是一样的。每张图片低于33ms，超过常用的SIFT，SURF等。由于要求旋转不变性，排除了BRIEF和LDB。

   - ORB，FAST角，256位描述子

2. 三个线程：tracking，local mapping，loop closing

   - tracking：用每帧定位相机并决定何时插入一个新的帧。先用之前的帧做初始特征匹配再用*motion-only* BA优化位姿。若tracking丢失，使用place recognition模块提供一个全局重定位。一旦有了一个初始的相机位姿和特征匹配的初始估计，将产生一个局部可视地图。这个局部地图使用系统维护的关键帧covisibility graph生成。

     接着，使用重投影搜索局部地图点匹配，并再次用这些匹配点优化相机位姿。最后，tracking线程将决定是否插入一个新关键帧。

   - local mapping：处理新关键帧，进行*local BA*，以得到一个给定相机位姿下优化后的周围场景重建。

     - *New correspondences for unmatched ORB in the new keyframe are searched in connected keyframes in the covisibility graph to triangulate new points* 为了三角化新点， 搜索covisibility graph中的相关帧，希望得到新帧中未匹配的ORB的新的响应。
     - 基于tracking线程收集的数据，筛选出高品质的点和非冗余关键帧。

   - loop closing：用新关键帧搜索loop（回环）

     - 检测到回环时，计算一个相似变换（similarity transformation）来说明回环中积累的drift。
     - 接着对齐回环两端并融合重复点
     - 最后执行一个服从相似约束（similarity constrain）的位姿图优化——Essential graph

   - 所有优化都将使用g2o库中的Levenberg-Marquardt算法

3. MapPoint和Keyframe的选择

   - 地图点$p_i$信息：
     1. 世界坐标系下的3D位置$X_{w,i}$
     2. viewing direction  $n_i$，它的所有观察方向的平均单位向量。观察方向指的是连接该地图点和光心的射线方向；
     3. ORB描述子$D_i$，在所有能观察到该点的帧中具有最小hamming距离的描述子；
     4. 能观察到该点的最大和最小距离$d_{max}, d_{min}$，由ORB特征尺度不变性限制
   - 关键帧$K_i$信息：
     1. 相机位姿$T_{iw}$，将世界坐标系变换为相机坐标系的刚体变换；
     2. 相机内参（intrinsic），包括焦距和pricipal point
     3. 所有从该帧中提取的ORB特征，可能与一个地图点相关也可能不相关（?）；若提供畸变模型，特征的位置是无畸变的。
   - create generously, culling

4. Covisobility Graph & Essential gsraph

   - covisibility graph 一个无向有权图，以关键帧为节点，若两帧共享至少15个相同地图点的观测，则两节点间有边且权值为共有地图点数。
   - 使用位姿图优化（pose graph optimization）来纠正回环，将loop closing error分布在图中。
   - essential graph：包括一个生成树，covisibility graph边的子集，闭环边。从初始关键帧中逐渐建立一个生成树，得到一个covisibility graph的最小连通子图。当插入一个新的帧时，它将连接在共享最多观测的树节点上，删除时也将更新树。

5. 词袋

   - 描述子空间的离散化，由大量图片中得到的ORB描述子构建，作为数据库先验，逆向索引，查询效率高。
   - 由于关键帧有视觉重叠，查询树的响应往往不单一。orb-slam的处理方式是将在covisibility graph中相连的关键帧分组，并返回得分高于最高分的75%的虽有关键帧；

   

**全自动图初始化**

- 二维场景：单应性 homography；非二维场景：基础矩阵（fundamental matrix）

- 算法步骤

  1. Find initial correspondences：在当前帧$F_c$中提取ORB特征，在参考帧$F_r$中查找匹配$x_c\leftrightarrow x_r$。若匹配数量不足，重置参考帧。

  2. Parallel computation of the two models：在两个平行线程中，使用标准DLT（直接线性变换）和八点法，分布计算单应矩阵$H_{cr}$和基础矩阵$F_{cr}$满足
     $$
     x_c = H_{cr}x_r,\ x_c^TF_{cr}x_r = 0
     $$
     为了两个线程处理的一致性，迭代次数、匹配点和个数都是相同的：基础矩阵使用八个点，其中4个点用于单应矩阵。在每次迭代时，计算模型M的分数$S_M$：
     $$
     S_M = \sum_i(\rho_M(d_{cr}^2(x_c^i,x_r^i,M))+\rho_M(d_{rc}^2(x_c^i,x_r^i,M)))
     $$

     $$
     \rho_M(d^2) = \left\{\begin{matrix}
     \Gamma -d^2,&d^2<T_M\\ 
     0,&d^2\geq T_M
     \end{matrix}\right.
     $$

     其中$d_{cr}^2$$d_{rc}^2$分别为从一帧到另外一帧的对称变换误差；$T_M$为拒绝outliner的阈值，基于95%卡方检验决定（$T_H$ = 5.99, $T_F = 3084$）；$\Gamma$数值上等于$T_H$，则在两个模型inliner区域中，对于相同d的打分会相等。

  3. Model Selection：

     - 单应矩阵和基础矩阵各有优缺点；由于重建方法（reconstruction method）能够正确地从一个平面初始化或者它可以检测视差较小的情况并拒绝初始化，应该选择单应矩阵。另一方面，一个具有足够视差的非二维场景仅能用基础矩阵表达，虽然当某些匹配点处在同一平面或者有较小视差的时候，单应矩阵也能够描述它们。
     - 我们设定一个自适应阈值$R_H$来决定何时应该选择哪一个模型，当$R_H>0.40$时选择单应矩阵，因为此时已经捕捉到了足够的二维或小视差匹配。否则选择基础矩阵。

     $$
     R_H = \frac{S_H}{S_H+S_F}
     $$

     ​	

  4. Motion and structure from motion recovery

     选择了模型之后，我们就得到了运动状态的假设。

     - 在单应矩阵的情况下，我们使用*论文23的方法提取8个运动假设。这个假设通过测试来选择有效方案。如果在低视差下云点跑到相机的前面或者后面，测试容易因为选择了一个错误方案而无效。我们建议**直接三角化这八个方案，检查在两个相机前和较少重投影误差的情况下，是否存在一个在视差下有最多云点的方案。如果没有一个明确的最佳方案，我们放弃初始化并回到步骤1**。这个消除方案的机制使得低视差和双重模糊配置下的初始化变得鲁棒，被认为是我们方法鲁棒性的关键。

     - 在基础矩阵的情况下，我们使用标定矩阵将基础矩阵转换为一个本质矩阵：
       $$
       E_{rc} = K^TF_{rc}K
       $$
       并用论文[2]的单值分解方法提取4个运动假设。我们三角化着4个方案并像处理单应矩阵情况一样选择是否重新初始化。

   5. Bundle adjustment：最后我们通过全BA来优化初始化重构



**追踪** **tracking**

tracking线程：处理来自相机的每一帧，优化相机位姿。

1. ORB 特征提取

   - 确定一幅图的角点数量：在8层金字塔提取FAST角点，尺度因子为1.2。对于图像分辨率重$512\times 384$到$752\times 480$，我们认为**1000**个角点是较为合适的。对于比这个范围高的分辨率，例如论文[14]中的KITTI数据集的分辨率为$1241\times 376$，我们提取2000个角点。为了确保角点均匀分布，我们将每个尺度层分成网格，每个网格提取至少5个角点。
   - 在每个网格检测角点。如果没有检测到足够的角点，检测器的阈值将自动调整。如果某些网格不存在角点（如缺乏纹理），单个网格的角点数量也会调整。
   - 计算FAST角点的orientation和ORB描述子。

2. 通过之前的帧初始化位姿估计

   - 若对上一帧的追踪成功，我们使用一个**匀速运动模型**来预测相机位，搜索上一帧观测到的地图点云。若没有足够的匹配，我们加大搜索范围搜索上一帧地图云点附近的点。通过寻找到的对应关系优化相机位姿。

3. 通过全局重定位初始化位姿估计

   - 若追踪丢失，我们把当前帧转换为词袋，查询识别数据库得到全局重定位的候选帧。计算ORB特征和每个关键帧中的地图云点的对应关系。对每个关键帧执行RANSAC迭代计算，用PnP算法找相机位姿。如果有足够的inliers确定相机位姿，我们优化该位姿并在候选帧的地图点云中搜索更多匹配，再一次优化位姿。如果再次地，有足够的inliers，继续追踪过程。

4. 追踪局部地图

   一旦有了一个相机位姿估计和特征匹配的初始集，我们就可以将地图投影到帧中并搜索更多地图点对应关系。为了限制大型地图的复杂度，只投影局部地图。这个局部地图包含了关键帧集$K_1$$K_2$$K_{ref}$。$K_1$是与当前帧有共同地图云点的关键帧组成的集合，$K_2$是在covisibility graph中与$K_1$相邻的关键帧组成的集合，$K_{ref}$是$K_1$的子集，由与当前帧有最多的共享云点的关键帧组成。

   搜索每个当前帧中，$K_1$$K_2$能“看到”的云点算法如下：

   1. 计算当前帧中地图云点投影x。如果超出范围则丢弃该点。
   2. 计算当前观察方向v和地图点平均观察方向n。如果$v\cdot n<cos60^{\c}$则丢弃该点。
   3. 计算地图云点到相机中心的距离$d$。如果$d\notin [d_{min},d_{max}]$，即它不在云点尺度不变区间，则丢弃该点。
   4. 计算每帧图像的尺度因子，$d/d{min}$
   5. 对比地图云点的特征描述子D和当前帧红还未匹配的ORB特征，在预测尺度，临近x和相关地图云点中做最佳匹配。

5. 新关键帧的判断标准

   最后一步是，确定当前帧是否可以作为关键帧。在（下一步）局部建图中有筛选冗余关键帧的机制。这一步的要求是尽可能快地插入新帧，这可以使得tracking thread对相机运动的计算（尤其是旋转）更具有鲁棒性。

   插入一个新关键需要满足如下要求：

   1. 距离上一次全局重定位至少20帧
   2. 局部建图线程处于空闲状态，或者距离上一次插入关键至少20帧；如果需要关键帧插入（过了20帧）而LocalMapping线程忙，则发送信号给LocalMapping线程，停止局部地图优化，使得新的关键帧可以被及时处理（20帧，大概过去了不到1s）
   3. 当前帧追踪至少50个点。确保了跟踪定位的精确度
   4. 当前帧追踪到参考关键帧的地图点数量少于90%。确保关键帧之间有明显的视觉变化。



**局部建图**

local mapping线程处理每个新关键帧$K_i$

1. 插入关键帧

   - 更新covisibility graph，添加一个新的节点表示$K_i$，更新边（连接$K_i$节点和共享地图点的其他关键帧）。
   - 更新生成树，将$K_i$连接到有最多共有点的帧的节点上。
   - 计算代表$K_i$帧的词袋（帮助三角化新点的数据关联）

2. 最近地图云点筛选

   为了将点保存在地图中，最初三个关键帧创建之后，地图点需要经过一个严格的测试以保证它们可被跟踪且正确三角化了。要求：

   1. 跟踪必须找到在多于25%的帧中找到这个点；说明这个点可以视作可见的。
   2. 如果多于一个关键帧被地图云点创建传递过来

3. 创建新地图云点

4. 局部Bundle Adjustment

5. 局部关键帧筛选

**回环检测**

loop closing线程处理local mapping线程处理的最后一个关键帧，并检测回环。



### 2. 源码分析

#### 单目初始化

- 系统运行

  ![1562748226909](.\assets\1562748226909.png)

- example-main函数

  - 主函数负责提取数据集中的图像，创建SLAM系统对象，调用Syatem::TrackMonocular，配合时间戳循环地把图像传给Tracker，并统计跟踪所花的时间的中值和平均值。

- **System类**

  - **构造函数**

    *完成了对整个项目的资源配置*，其中包括

    1. 读入配置文件
    2. 读取orb词袋（用于定位和特征匹配）
    3. 创建关键帧数据库（用于定位和闭环检测）
    4. 创建地图对象（用于储存KeyFrames 和 MapPoints的指针）
    5. 创建用于可视化的图像窗口mpFrameDrawer和轨迹窗口mpMapDrawer
    6. 创建tracking对象Tracking构造函数，主线程
    7. 创建mpLocalMapper对象，副线程1，运行ORB_SLAM2::LocalMapping::Run函数，通过BA来管理本地地图
    8. 创建LoopClosing对象，副线程2，运行ORB_SLAM2::LoopClosing::Run函数，寻找闭环，当找到闭环时优化位姿，并执行全局BA优化
    9. 创建Viewer对象，副线程3，运行Viewer::Run函数（做图）

  - System类-**TrackMonocular**

    - 跟踪单目函数，传入一张图片和对应时间戳，调用tracker的GrabImageMonocular函数，返回位姿Tcw：

- **Tracking 线程**

  - **两种模式**：仅追踪模式和同时定位与建图模式

    - 仅追踪模式下，只进行追踪地图中现有的地图点，不插入新的关键帧，不添加新的地图点，局部建图和回环检测线程都不工作
    - 同时定位与建图模式：tracking同时局部建图和回环检测。

  - **五种追踪状态**：当前状态mState和上一个状态mLastProcessedState，枚举项分别为"系统未准备好"/“无图像"/“未初始化”/“跟踪正常”/“跟踪丢失。（一般只用到4后四种）

    ![1562748396579](.\assets\1562748396579.png)

  - Tracking 线程-**创建**

    1. 初始化系统状态  mState \leftarrow NO_IMAGES_YET

    2. 将system传过来的各项个对象指针进行配置，包括

       > 传感器 mSenser
       > ORB词典 mpORBVocabulary
       > 关键帧数据库 mpKeyFrameDB
       > 系统  mpSystem
       > 地图 mpMap
       > 图像画布 mpFrameDrawer
       > 地图画布 mpMapDrawer

    3. 从文件中加载

       > 1. 相机内参fx, fy, cx, cy
       > 2.  图像校正系数
       > 3.  帧率 fps
       > 4. ORB特征提取器所需要的相关参数，包括最多提取的特征点的数量nFeatures，尺度参数	fScaleFactor，金字塔层数nLevels，默认fast  角点检测的时候的阈值fIniThFAST，最小	的fast特征检测阈值fMinThFAST

     4. 配置特征提取器：`ORBextractor`的构造函数。`mpORBextractorLeft`是tracking过程中都会用到的，而`mpIniORBextractor`用于单目初始化，后者的提取数量是前者的两倍。

  - 进入tracking

    ![1562748642148](.\assets\1562748642148.png)

- ==**单目初始化**==

  1. 选取两个可以作为起始两帧的初始帧

     - 条件：相邻两帧的特征数点都大于100且这两帧的匹配点数大于100 

       ![1562748687996](.\assets\1562748687996.png)

  2. 根据匹配计算两帧之间的位姿  

     *现在我们具备了两张相邻图像中的一组匹配好的特征点，通过这些二维图像点的对应关系，我们来恢复两帧之间摄像机的运动。这个操作我们通过初始器mpInitializer中的Initialize函数实现*

     - 目标：通过两帧图像中八对匹配点求解旋转矩阵R和平移向量t

     - 原理：对极约束

     - **根据匹配点的像素位置，求矩阵E或者F，从矩阵E或者F中还原出R和t**

     - E和H的选择：设定分数

       ![1562749496097](.\assets\1562749496097.png)

       ![1562749502207](.\assets\1562749502207.png)

       ![1562749533023](.\assets\1562749533023.png)

       > 单应矩阵和基础矩阵各有优缺点；
       >
       > - 为了使得重建方法（reconstruction method）能够正确地从一个平面初始化或者它可以检测视差较小的情况并拒绝初始化，应该选择单应矩阵。
       > - 另一方面，一个具有足够视差的非二维场景仅能用基础矩阵表达，虽然当某些匹配点处在同一平面或者有较小视差的时候，单应矩阵也能够描述它们

     - 算法

       > 1. 在当前帧中提取ORB特征，在参考帧中查找匹配。若匹配数量不足，重置参考帧。
       > 2. 在所有匹配特征点对中随机选取8对匹配点为一组用于F和H矩阵求解
       > 3. 创建两个线程，使用标准DLT（直接线性变换）和八点法，同时计算H和F矩阵
       > 4. 计算模型分数
       > 5. 设定一个自适应阈值R_H来决定何时应该选择哪一个模型，当R_H>0.40时选择单应矩阵，因为此时已经捕捉到了足够的二维或小视差匹配。否则选择基础矩阵。
       > 6. 从选择的模型中分解出旋转矩阵R和平移向量t，还原相机运动；

#### 建图

**该地图点被哪些关键帧观测到，对应的哪个（idx）特征点**

跟踪线程

- **三角化**：在ReconstructH对t有归一化，3D点的深度取决于t（平移向量）的尺度，此处恢复的3D点并没有决定整个单目SLAM 中的尺度，会在后面初始化地图的时候对深度进行缩放，反过来改变t的数值。

  | 符号     |                           |                                                |
  | -------- | ------------------------- | ---------------------------------------------- |
  | $P_w$    | $[X,Y,Z,1]^T$             | 三维空间点                                     |
  | $T_{rw}$ | $[R_{rw}，t_{rw}]$        | 参考帧相机位姿                                 |
  | $T_{cw}$ | $[R_{cw}，t_{cw}]$        | 当前帧相机位姿                                 |
  | $p_r$    | $[x_r/z_r, y_r/z_r, 1]^T$ | $P_w$在参考帧归一化平面里的位置，深度为$z_c$   |
  | $p_c$    | $[x_c/z_c, y_c/z_c, 1]^T$ | $P_w$在当前帧归一化平面里的位置，，深度为$z_c$ |

  所以有
  $$
  \begin{align}
  z_rp_r = T_{rw}P_w\\z_cp_c = T_{cw}P_w
  \end{align}
  $$
  对应归一化平面的点取外积，整理得到
  $$
  \begin{bmatrix}
  p_r T_{rw}\\
  p_c T_{cw}\\
  \end{bmatrix} P_w= Ax = 0
  $$
  即通过一个其次方程来求解空间点$P_w$，其中矩阵$A\isin R^{6*4}$是超定方程，所以目标函数为

  $J(x)=min||Ax||$，使用Opencv提供的SVD分解求解，最后将得到的其次向量进行归一化；

  ```c++
  void Initializer::Triangulate(const cv::KeyPoint &kp1, const cv::KeyPoint &kp2, const cv::Mat &P1, const cv::Mat &P2, cv::Mat &x3D)
  {
      cv::Mat A(4,4,CV_32F);
  
      A.row(0) = kp1.pt.x*P1.row(2)-P1.row(0);
      A.row(1) = kp1.pt.y*P1.row(2)-P1.row(1);
      A.row(2) = kp2.pt.x*P2.row(2)-P2.row(0);
      A.row(3) = kp2.pt.y*P2.row(2)-P2.row(1);
  
      cv::Mat u,w,vt;
      cv::SVD::compute(A,w,u,vt,cv::SVD::MODIFY_A| cv::SVD::FULL_UV);
      x3D = vt.row(3).t();
      x3D = x3D.rowRange(0,3)/x3D.at<float>(3);
  }
  ```

  打印一个中间过程：

  

- **创建初始地图**:CreateInitialMapMonocular

  > 将三角化得到的3D点包装成MapPoints类型存入KeyFrame和Map对象中

  1. 将初始参考帧和当前帧构造成关键帧，即用（之前创建的）Frame、3D点，关键帧数据库在关键帧中注册。

  2. 计算BoW

  3. 将这两个关键帧插入地图

     1. 用关键帧中的3D点构造MapPoint对象
     2. 将MapPoint加入关键帧，表示关键帧的哪个特征点可以观察到哪个3D点
     3. 加入观察Observation，表示该MapPoint可以被哪个KeyFrame的哪个特征点观测到；
     4. 从众多观测到该MapPoint的特征点中挑选区分读最高的描述子（最好的描述子与其他描述子的距离的中值最小）
     5. 更新平均观测方向以及观测距离范围
        - 平均观测方向：获得观测到该3d点的所有关键帧，对所有关键帧对该点的观测方向归一化为单位向量进行求和，除以所有关键帧数；
        - 观测距离范围
     6. 在Map中添加Mapoint

  4. 更新关键帧之间的图Connection

     1. 获得该关键帧的所有MapPoint。

     2. 计算观测到某一MapPoint的每个关键帧与其他所有关键帧的共视程度，对每个共视达到一定程度（15）的关键帧建立一条边，边的权重是公共3D点的个数。（如果没有超过阈值的权重，则对权重最大的关键帧建立连接）
     3. 其他关键帧的权重数组

  5. 全局BA：最小化重投影误差，并将优化后的位姿返回给参与优化的帧。

  6. 把初始关键帧和当前关键帧给局部地图线程，所有的地图点给跟踪器的局部关键点

- 

建图线程



MapPoint 与 Map

为了更好地理解建图的内容，我摘取了各个线程对于MapPoint和Map对象的创建和维护部分来阐述ORB-SLAM中地图的部分。

- 地图点的创建
  - MapPoint可以通过关键帧构造也可以通过普通帧构造，但通过后者构造的地图点只是临时被用于追踪。

  1. **三角化**：恢复深度信息

  2. 用关键帧中的3D点构造MapPoint对象

  3. 将MapPoint加入关键帧，表示关键帧的哪个特征点可以观察到哪个3D点

  4. 加入观察Observation，表示该MapPoint可以被哪个KeyFrame的哪个特征点观测到；

  5. 计算地图点描述子：从mObservations获得众多观测到该MapPoint的关键帧及对应描述子，挑选区分读最高的描述子（中值距离最小）作为地图点描述子的`mDescriptor`
  6. 更新平均观测方向以及观测深度范围
     - 平均观测方向：获得观测到该3d点的所有关键帧，对所有关键帧对该点的观测方向归一化为单位向量进行求和，除以所有关键帧数；
     - 深度范围：地图点到**参考帧**（只有一帧）相机中心距离，乘上参考帧中描述子获取时金字塔放大尺度，得到最大距离mfMaxDistance；最大距离除以整个金字塔最高层的放大尺度得到最小距离mfMinDistance。通常来说，距离较近的地图点，将在金字塔层数较高的地方提取出，距离较远的地图点，在金字塔层数较低的地方提取出（金字塔层数越低，分辨率越高，才能识别出远点）。因此通过地图点的信息（主要是对应描述子），我们可以获得该地图点对应的金字塔层级，从而预测该地图点在什么距离范围内能够被观测到

  7. 在Map中添加Mapoint
- 地图点与关键帧的 观测关系
  - 地图点最终必须是和关键帧对应的。**该地图点被哪些关键帧观测到，对应于哪一个特征点**。通过MapPoint类的两个成员维护：

    ```c++
    std::map<KeyFrame*,size_t> mObservations;
    // 观测到该地图点的相机数
    int nObs;
    ```

  - 添加地图点的观测：能够观测到同一个地图点的关键帧之间存在共视关系（covisibility）

    ```c++
    void MapPoint::AddObservation(KeyFrame* pKF, size_t idx);
    ```

    

  - 删除地图点观测：从当前地图点的mObservation和nObs成员中删掉对应关键帧**观测关系**

    ```c++
    void MapPoint::EraseObservation(KeyFrame* pKF);
    ```

    

  - 删除地图点：**当观测相机数小于等于2时，该地图点需要剔除。**对应地，删掉关键帧中对应的地图点以及Map中该地图点

  - **替换**地图点

## 3. 数学、算法基础

*主要来自于《视觉SLAM十四讲》一书的学习过程及相关博客和代码*

### ch3 三维运动

- 数学描述
  - 离散时间：
  - 机器人位置：
  - 机器人是从上一个时刻运动到下一个时刻的
  - **运动方程**：

  $$
  x_k = f(x_{k-1}, u_k, w_k)\tag{1.1}
  $$

  - 路标（三维空间点）：$y_1, y_2...,y_n$

  - 传感器在位置x_k处探测到了路标y_j

  - **观测方程**：
$$
  z_{k,j} = h(x_k, y_j, v_{k,j})\tag{1.2}
$$

  - 问题：

    - 位置是三维的，如何描述？
    - 观测是相机中的像素点，如何表述？
    - 已知$u,z$，如何推断$x,y$？



- 旋转矩阵

  1. 点，向量和坐标

     - 一个三维空间中的某个向量$a$的坐标可以用一个线性空间的基表示：
       $$
     a = [e_1, e_2, e_2]\begin{bmatrix}
     a_1\\ 
     a_2\\ 
     a_3
     \end{bmatrix} = a_1e_1+a_2e_2+a_3e_3
       $$

     - 内积：描述向量之间的投影关系
       $$
     a \cdot b = a^Tb=\sum^{3}_{i=1}a_ib_i=|a||b|cos
     \left \langle a,b \right \rangle
       $$

     - 外积：方向垂直于两个向量，大小为$|a||b|sin\left \langle  a,b \right \rangle$，是两个向量张成的四边形的有向面积，记为$a^\wedge b$。
       $$
     a \times b = \begin{bmatrix}
     i&j&k\\
     a_1&a_2&a_3\\
     b_1&b_2&b_3\\
     \end{bmatrix} = \begin{bmatrix}
     a_2b_3-a_3b_2\\
     a_3b_1-a_1b_3\\
     a_1b_2-a_2b_1\\
     \end{bmatrix} = \begin{bmatrix}
     0&-a_3&a_2\\
     a_3&0&-a_1\\
     -a_2&a_1&0
     \end{bmatrix}b \\ = a^\wedge b\tag{2.1}
       $$

     - 用外积表示旋转：考虑空间中两个不平行的向量a和b，根据右手系，将四指从a转向b，大拇指朝向就是旋转向量的方向，旋转向量的大小由ab的模长和夹角大小决定。

     

  2. 世界坐标系$p_w$和摄影坐标系$p_c$间的欧式变换

     - **旋转矩阵推导**：相机运行是一个刚体运动，并保证同一个向量在各个坐标系$e$和$e'$下的长度和夹角都不会发生变化，这种变换称为欧式变换：
       $$
     [e_1, e_2, e_3]\begin{bmatrix}
     a_1\\ 
     a_2\\ 
     a_3
     \end{bmatrix} = [e_1', e_2', e_3']\begin{bmatrix}
     a_1'\\ 
     a_2'\\ 
     a_3'
     \end{bmatrix}
       $$
     同时左乘$\begin{bmatrix}
     e_1^T\\ 
     e_2^T\\ 
     e_3^T
     \end{bmatrix}$得到旋转矩阵$R$，由**两组基之间的内积组成**：
     $$
     \begin{bmatrix}
     a_1\\ 
     a_2\\ 
     a_3
     \end{bmatrix}=\begin{bmatrix}
     e_1^Te_1'&e_1^Te_2'&e_1^Te_3'\\
     e_2^Te_1'&e_2^Te_2'&e_2^Te_3'\\
     e_3^Te_1'&e_3^Te_2'&e_3^Te_3'
     \end{bmatrix}\begin{bmatrix}
     a_1\\ 
     a_2\\ 
     a_3
     \end{bmatrix}=Ra'
     $$

     - **旋转矩阵的性质**：行列式为1的正交矩阵（充分必要）*【特殊正交群】*
       $$
       SO(n) = \{R \isin  \mathbb{R}^{n\times n}|RR^T = I,det(R) = 1\}
       $$

     - **旋转加平移**
       $$
       a' = Ra +t\tag{2.2}
       $$

  3. 变换矩阵和齐次坐标

     - 由于式(2.1)表达的不是一个线性关系，当出现两个及以上的连续变换的复合，形式会过于复杂。

     - **齐次坐标**：
       $$
     \begin{bmatrix}
     a'\\
     1
     \end{bmatrix}=\begin{bmatrix}
     R&t\\
     0^T&1
     \end{bmatrix}\begin{bmatrix}
     a\\
     1
     \end{bmatrix}=T\tilde{a}\tag{2.3}
       $$

     - **变换矩阵**$T$：左上角为旋转矩阵，右侧为平移向量*【特殊欧式群】*
       $$
     SE(3) = \left \{T =\begin{bmatrix}
     R&t\\
     0^T&1
     \end{bmatrix}\isin \mathbb{R}^{4\times 4} | r\isin S0(3),T\isin \mathbb{R}^{3})\right \}
       $$



- 旋转向量和欧拉角

  1. 旋转向量：方向与旋转轴一一致，长度等于旋转角  **李代数**

     - 旋转向量和旋转矩阵之间的转换：**罗德里格斯公式**

     $$
     R = cos\theta I+(1-cos\theta)nn^T+sin\theta n^\wedge
     $$

   ​	where:

   ​		n : 旋转轴

   ​		$\theta$ : 旋转角

   ​		 $^\wedge$ : 向量到反对称的转换符（见$(2.1)$）

     - 旋转矩阵转旋转向量：

  $$
   \begin{align}
   tr(R) &= cos\theta tr(T) + (1-cos\theta tr(nn^T)+sin\theta tr(n^\wedge))\\
   &=1+2cos\theta
   \end{align}
  $$

   ​	由此可得
  $$
   \theta = arccos(\frac{tr(R) -1}{2})
  $$

     - 对于旋转轴n，由于旋转轴方向上的向量在旋转后不发生改变，即$Rn=n$，说明转轴n是**旋转矩阵R特征值1对应的特征向量**（快去补线性代数！）。

  2. 欧拉角

     - 将一个旋转分解为三次绕不同轴的旋转。

     > ZYX转角如下
     >
     > 1. 绕物体分Z轴旋转 $\rightarrow$ 偏航角 yaw
     > 2. 绕**旋转之后Y轴**旋转 $\rightarrow$俯仰角pitch
     > 3. 绕**旋转之后X轴**旋转 $\rightarrow$滚转角roll

     得到$[r,p,y]^T$

   - 万向锁问题：奇异性

- 四元数

  1. 表示

     - Quaternion
       $$
     q = q_0+q_1i+q_2j+q_3k\tag{3.1}
       $$
     其中三个虚部$i,j,k$为满足关系式
       $$
     \left\{\begin{matrix}
     i^2 = j^2=k^2=-1\\ 
     ij=k,ji=-k\\ 
     jk = i,kj=-i\\ 
     ki=j,ik=-j
     \end{matrix}\right.\tag{3.2}
       $$
     或者用一个标量和一个向量表示：
       $$
     q=[s,v],s=q_0\isin \mathbb{R},v=[q_1,q_2,q_3]^T\isin \mathbb{R}^3\tag{3.3}
       $$

     - 用单位四元数表示三维空间中任意一个旋转：设某个旋转绕单位向量$n=[n_x,n_y,n_z]^T$进行了角度为$\theta$的旋转，那么这个旋转的四元数形式为
       $$
     a=[cos\frac{\theta}{2},n_xsin\frac{\theta}{2},n_ysin\frac{\theta}{2},n_z\frac{\theta}{2}]^T\tag{3.4}
       $$
     也可以从单位四元数中计算对应的旋转轴与夹角：
       $$
     \left\{\begin{matrix}
     &\theta = 2arccosq_0\\ 
     &[n_x,n_y,n_z]^T = [q_1,q_2,q_3]^T/sin\frac{\theta}{2}\\
     \end{matrix}\right.\tag{3.5}
       $$

  2. 运算

     1. 加减法  

  $$
   q_a\pm q_b = [s_a\pm s_b,v_a\pm v_b]
  $$

     2. 乘法：将$q_a$每一项与$q_b$每一项相乘最后相加并整理==公式==

   ​	通过向量的内外积运算形式：==公式==                                                      

     3. 共轭
        $$
        q^* = s_a-x_ai-y_aj-z_ak = [s_a,-v_a]
        $$
      
        - 性质
          $$
          q^*q =qq^*=[s_a^2+v^Tv,0]
          $$

     4. 求逆
        $$
        q^{-1}= q^* / ||q||^2
        $$
      
        - 性质 1：$qq^{-1} = q^{-1}q = 1$
        - 性质2：如果q是单位四元数，逆和共轭是是相同
        - 性质3：乘积的逆等于逆的乘积（注意下标）$(q_aq_b)^{-1} = q_b^{-1}q_a^{-1}

     5. 模长
        $$
        |q_a| = \sqrt{s_a^2+x_a^2+y_a^2+z_a^2}
        $$

     6. 数乘
        $$
         kq     = [ks,kv]
        $$

     7. 点乘
        $$
        q_a\cdot q_b=s_as_b+x_ax_bi+y_ay_bj+z_az_bk
        $$



  3. 四元数表示旋转

     1. 用虚四元数表示三维空间点

     $$
     p=[0,x,y,z] = [0,v]
     $$

     2. 用四元数表示旋转：
$$
   q=[cos\frac{\theta}{2},nsin\frac{\theta}{2}]
$$

​	 3. 旋转后的$p'$表示为
$$
p'=qpq^{-1}
$$

4. 设四元数为$q = q_0+q_1i+q_2j+q_3k​$
      - 四元数到旋转矩阵:
      $$
        R = \begin{bmatrix}
        1-2-2&2+2&2-2\\
        2-2&1-2-2&2+2\\
        2+2&2-2&1-2-2
        \end{bmatrix}
      $$
    
      - 旋转矩阵到四元数：
      $$
      q_0 = \frac{\sqrt{tr(R)+1}}{2},q_1 = \frac{m_{23} -m_{32}}{4q_0},q_2 = \frac{m_{31}-m_{13}}{4q_0}, q_3 = \frac{m_{12} - m_{21}}{4q_0}
      $$

- 常见变换

| 名称 | 格式                                            | 自由度 | 不变性质             |
| ---- | ----------------------------------------------- | ------ | -------------------- |
| 欧式 | $T_E = \begin{bmatrix}R&t\\ 0^T&v\end{bmatrix}$ | 6      | 长度，夹角，体积     |
| 相似 | $T_S = \begin{bmatrix}sR&t\\0^T&1\end{bmatrix}$ | 7      | 体积比               |
| 仿射 | $T_A = \begin{bmatrix}A&t\\0^T&1\end{bmatrix}$  | 12     | 平行性，体积比       |
| 投影 | $T_P = \begin{bmatrix}A&t\\a^T&v\end{bmatrix}$  | 15     | 接触平面的相交和相切 |





### ch4 李代数基础



回顾一下， **特殊正交群**和**特殊欧式群**分别是一下两个东西：
$$
SO(3) = \{R\isin \mathbb{R}^{3\times 3}|RR^T = I,det(R) = 1\}
$$

$$
SE(3) = \left \{T =\begin{bmatrix}
R&t\\
0^T&1
\end{bmatrix}\isin \mathbb{R}^{4\times 4} | r\isin S0(3),T\isin \mathbb{R}^{3})\right \}
$$

1. 群

   - **群**（Group）是一种集合加上一种运算的代数结构。令$G$是集合$A$上的群，可以记作$G = (A,\cdot)$，且群要求这个运算满足以下几个条件：
     1. 封闭性：
     2. 结合律：
     3. 幺元：
     4. 逆：
   - **李群**：连续（光滑）的群

2. 李代数的推导

   考虑某个相机的旋转矩阵$R$随着时间连续变化记为$R(t)$且满足
   $$
   R(t)R(t)^T = I
   $$
   等式两边对时间$t$求导并整理可得
   $$
   \dot{R}(t)R(t)^T  =-( R(t)\dot{R}(t)^T )^T\tag{1}
   $$
   可见$\dot{R}(t)R(t)^T$是一个**反对称矩阵**。

   > 反对称矩阵：n维方阵有$A' = -A$。主对角线上的元素全为零，且位于主对角两侧对称的元符号相反。之前引入过符号$\wedge $将一个向量a转换为相应的反对称矩阵，即$a^{\wedge } = \begin{bmatrix}0&-a_3&a_2\\a_3&0&a_1\\-a_2&a_1&0\end{bmatrix}$

   那么就存在一个三维向量$\phi(t)$与之对应，即$\dot{R}(t)R(t)^T=\phi(t)^{\wedge}$。

   两边右乘上$R(t)$，根据旋转矩阵R正交的性质有$R^TR = I$，于是得到
   $$
   \dot{R}(t) = \phi(t)^{\wedge}R(t)\tag{2}
   $$
   这个式子说明，**每对旋转矩阵R求一次导，只需要左乘一个$\phi^{\wedge}$矩阵**。

   

   ⭐️ 接下来取$t_0 = 0，R(0)=I$的情况来说明$t \rightarrow0$的问题：

   - 按照导数的定义，可以将$R(t)$在0附近进行一阶泰勒展开：
     $$
     \begin{align}R(t) &\approx R(t_0)+\dot{R}(t_0)(t-t_0)\\
     &=I+\phi(t_0)^{\wedge}(t)
     \end{align}\tag{3}
     $$

   - 令$t_0$附近，$\phi$保持为常数，根据式(2)有
     $$
     \dot{R}(t) =\phi(t)^{\wedge}R(t)=\phi_0^{\wedge}R(t)
     $$

   - 得到关于$R$的微分方程，且初始值$R(t_0) = I$，解这个微分方程得到
     $$
     R(t)= exp(\phi_0^{\wedge}t)
     $$

   虽然上式只在$t=0$附近有效，但根据这个式子可以得到，对某个时刻的R，存在一个反对称矩阵$\phi$满足**指数关系**。

3. 李代数

   - 李代数描述了对应的李群的局部性质。

   - 李代数的三元表示：$\mathfrak{g}=(V,F,[\ ])$，其中集合$V$,数域$F$和二元运算（李括号）$[\ ]$

   - 李代数性质

     1. 封闭性
     2. 双线性
     3. 自反性
     4. 雅克比等价

   - 设$\mathfrak{s}\mathfrak{o}(3)$是与$SO(3)$对应的李代数
     $$
     \mathfrak{s}\mathfrak{o}(3)=\left\{
     \phi\isin \mathbb{R}^3, \Phi = \phi^{\wedge}\isin\mathbb{R}^{3\times3}
     \}\right.
     $$
     李括号为
     $$
     [\phi_1,\phi_2]=(\phi_1^{\wedge}\phi_2^{\wedge} - \phi_2^{\wedge}\phi_1^{\wedge})^{\vee}
     $$
     和$R\isin SO(3)$有指数对应关系
     $$
     R = exp(\phi^{\wedge})
     $$

   - 设$\mathfrak{s}\mathfrak{e}(3)$是与$SE(3)$对应的李代数
     $$
     \mathfrak{s}\mathfrak{e}(3) = \left \{ \xi = \begin{bmatrix}\rho\\\phi \end{bmatrix}\isin \mathbb{R}^6, \rho\isin\mathbb{R}^3,\phi\isin \mathfrak{s}\mathfrak{o}(3),\xi^{\wedge} = \begin{bmatrix}\phi^{\wedge}&\rho\\0^T&0\\\end{bmatrix}\isin \mathbb{R}^{4\times4} \right \}
     $$
     李括号为
     $$
     [\xi_1,\xi_2]=(\xi_1^{\wedge}\xi_2^{\wedge} - \xi_2^{\wedge}\xi_1^{\wedge})^{\vee}
     $$



### ch5 相机成像模型

**单目相机**：**针孔模型**

- 几何建模

:eight_pointed_black_star: 符号集

| 符号                 | 表格                                   |
| -------------------- | -------------------------------------- |
| $O$                  | 相机光心                               |
| $O-x-y-z$            | 相机坐标系                             |
| $O-x'-y'$            | 物理成像平面                           |
| $o-u-v, [u,v]^T$     | 像素平面（固定在物理成像平面上）及坐标 |
| $P,\ [X,Y,Z]^T$      | 相机坐标系下空间点和坐标               |
| $P', \ [X',Y',Z']^T$ | 成像点和成像平面坐标                   |
| $f$                  | 焦距，成像平面到小孔点                 |
| $R$                  | 旋转矩阵                               |
| $t$                  | 平移向量                               |

✴️ 推导

**相机坐标系与成像坐标关系**：根据三角形相似关系，并将成像对称到相机前方（与三维空间点的同一侧）
$$
\frac{Z}{f} = \frac{X}{X'} = \frac{Y}{Y'}
$$
整理得到
$$
\begin{align}
    X' &= f\frac{X}{Z} \\
    Y'&= f\frac{Y}{Z}\\\tag{1.1}
    \end{align}
$$
又因为像素坐标系与成像平面之间相差了一个缩放和一个原点的平移：
$$
    \left\{\begin{matrix}
    u = \alpha X'+c_x\\
    v = \beta Y'+c_y
    \end{matrix} \right.\\\tag{1.2}
$$
将$(1.1)$带入$(1.2)$得到
$$
    \left\{\begin{matrix}
    u = f_x\frac{X}{Z}+c_x\\
    v = f_y\frac{Y}{Z}+c_y
    \end{matrix} \right.\\\tag{1.3}
$$
where $f_x = \alpha f, \ f_y = \beta f$，且$f$单位为米，$\alpha, \beta$的单位都是像素每米。将$(1.3)$写成左侧是齐次坐标的矩阵形式：
$$
    Z\begin{pmatrix}
    u \\ 
     v\\
     1
    \end{pmatrix}= \begin{pmatrix}
    f_x &0&c_x \\ 
     0&f_y&c_y \\
     0&0&1
    \end{pmatrix}\begin{pmatrix}
    X \\ 
    Y\\
    Z
    \end{pmatrix}=KP.
$$
由此我们引入了**内参矩阵 K**，一般是由相机本身决定的，而自己确定相机内参的过程称为**标定**

------



**世界坐标到像素坐标**：由于相机在运动，所以P的相机坐标是$P_w$根据相机当前的位姿变换到相机坐标系下的结果，而相机位姿由它的$R$和$t$（**外参**）描述：
$$
ZP_{uv} = Z\begin{bmatrix}
u\\
v\\
1
\end{bmatrix} =	KP= K(RP_w+t) = KTP_w
$$

> PTT: 右侧隐含了一个齐次坐标到非齐次坐标的转换，务必要弄清楚

**归一化平面**：相机前方$z=1$的平面，$P_c = \begin{bmatrix}X/Z\\ Y/Z\\ 1\end{bmatrix}$可以看做一个二维的其次坐标，由于经过内参之后得到像素坐标，可以把像素坐标视作对归一化平面上的点进行量化的结果。





### ch6 非线性优化 

讨论有噪声的状态中如何进行状态估计：最小二乘法的推导及其数值计算方法。



**状态估计问题**

经典的SLAM模型由一个运动方程和一个观测方程构成：
$$
\left\{\begin{matrix}
x_k = f(x_{k-1},u_k)+w_k\\ 
z_{k,j} = h(y_j,x_k)+v_{k,j}
\end{matrix}\right.
$$
其中

- $u_k$是输入数据，包括测量运动的传感器的数据（通常没有）；

- $x_k$是相机位姿，表示为变换矩阵$T_k$或李代数$exp(\xi^{\wedge}_k)$；

- $z_{k,j}$表示在$x_k$处对路标$y_j$进行一一次观测得到的像素坐标；

- $w_k, v_{k,j}$ 为噪声项，通常假设噪声项满足零均值的高斯分布，即
  $$
  w_k \sim N(0, R_k), v_k\sim N(0,Q_{k,j})
  $$

需要解决的问题是，如何通过带噪声的数据$z$和$u$推断位姿$x$和地图$y$。

**最大后验和最大似然**

进一步从概率论的角度描述这个问题。将所有待估计的变量放在状态变量（集合）中“
$$
x = {x_1,...,x_N,y_1...,y_M}
$$
**已知输入数据u和观测数据z，计算状态x的条件概率分布**:
$$
P(x|z,u)
$$
经常地，没有运动传感器的时候只考虑观测方程的影响。利用贝叶斯法则有
$$
P(x|z) = \frac{P(z|x)P(x)}{P(z)} \propto P(z|x)P(x)
$$
其中，$P(x|z)$为后验，$P(z|x)$为似然，$P(x)$为先验。直接求解后验概率分布比较困难，于是将问题转换成求一个状态最优估计，使得在该状态下后验概率最大化的**MAP**问题：
$$
x*_{MAP} = arg\ maxP(x|z) = arg\ max\ P(z|x)P(x)
$$
进一步说，当缺少机器人位置的先验知识，问题转化成求解x的最大似然估计**MLE**，即，**求解最可能产生当前观测数据的状态**：
$$
x*_{MLE} = arg\ max\ P(z|x)
$$

**最小二乘**



为了计算这个MLE，首先明确我们已经假设了噪声项服从高斯分布，于是有观测数据的条件概率，即，在位姿$x_k$观测路标$y_j$得到的观测数据$z_{k,j}$，服从以真实值$h$为均值，噪声项的方差为方差的**高斯分布**：
$$
P(z_{k,j}|x_k,y_j) = N(h(y_j, x_k), Q_{k,j} \tag{1}
$$

> 高斯分布的最大似然的求解往往使用**最小化负对数**的方式求解。
>
> 考虑任意高维的高斯分布$x\sim N(\mu, \Sigma)$，概率密度展开式（pdf）为
> $$
> P(x;\mu,\Sigma) = \frac{1}{\sqrt{(2\pi)^Ndet(\Sigma)}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))
> $$
> 取负对数，
> $$
> -ln(P(x;\mu, \Sigma)) = \frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu) + \frac{1}{2}ln((2\pi)^Ndet(\Sigma))
> $$

**对原分布求最大化相当于对负对数求最小化**。而且由于右边第二项与状态x无关，只需最小化右边第一项，于是带入SLAM模型（式1）得到
$$
x* = arg\ min((z_{k,j}-h_{(x_k, y_j)})^TQ^{-1}(z_{k,j}-h_{(x_k,y_j)}))
$$
上式等价于最小化观测方程的噪声项（误差）。现在将运动方程也考虑进来，定义测量值和真实值之间的误差为
$$
\begin{align}
e_{v,k}&=x_k-f(x_{k-1}, u_k)\\
e_{y,j,k}&=z_{k,j} - h(x_k, y_j)
\end{align}
$$
为了使系统整体的误差最小，取误差的平方和（二范数形式度量误差）：
$$
J(x) = \sum_{k}e_{v,k}^TR_{k}e_{v,k}+\sum_k\sum_j e_{y,j,k}^TQ_{k,j}e_{y,j,k}
$$

### 对极约束

![1562748732950](.\assets\1562748732950.png)

1. 假设现在在两帧图像中都观察到了特征点P，其世界坐标如下，在第一帧和第二帧中的像素坐标分别为p1，p2

   ![1562748895548](.\assets\1562748895548.png)

2. 根据相机针孔模型，像素坐标与世界坐标的转换如下式。为了方便计算使用齐次坐标，写成在乘以非零常数下成立的等式：

   ![1562748901225](.\assets\1562748901225.png)

   ![1562748907985](.\assets\1562748907985.png)

3. 令x1，x2为两个像素在归一化平面（ch5）上的坐标，根据定义：

   ![1562748934301](.\assets\1562748934301.png)

4. 将x1，x2带入(2)式

   ![1562748940572](.\assets\1562748940572.png)

5. 左乘t^

   ![1562748946136](.\assets\1562748946136.png)

6. 左乘x2的转置

   ![1562748950727](.\assets\1562748950727.png)

7. 代回p1，p2

   ![1562748958608](.\assets\1562748958608.png)

   ![1562748965571](.\assets\1562748965571.png)

   

   
### 本质矩阵和单应矩阵

- 本质矩阵E
    - 八点法
      
      ![1562749069534](.\assets\1562749069534.png)
      
    - SVD分解
      
      ![1562749101827](.\assets\1562749101827.png)
      
    - 多重解    
      ![1562749117012](.\assets\1562749117012.png)
   
- 单应矩阵H
  

![1562749135317](.\assets\1562749135317.png)
![1562749144953](.\assets\1562749144953.png)
![1562749155636](.\assets\1562749155636.png)
![1562749160653](.\assets\1562749160653.png)
![1562749171054](.\assets\1562749171054.png)
![1562749186297](.\assets\1562749186297.png)


## 4. 小实验

### 1. 拼接点云练习

**TASK**

给定5张RGB-D图像、每个图像的内参和外参以及5张图像对应的相机位姿。计算，任意像素在相机坐标系下的位置、任意像素在世界坐标系下的位置，并构建地图。

说明：

1. 位姿记录形式为平移向量加旋转四元数：$[x,y, z,q_x,q_y,q_z,q_w]$
2. 需要安装PCL，生成的点云以PCD格式存储在map.pcd中，使用PCL提供的可视化程序*pcl_viewer*打开。



**主程序分析**

*joinMap.cpp*

- 数据结构

  - colorImgs : vector\<cv::Mat\>

  - depthImgs : vector\<cv::Mat\>

  - poses : vector\<Eigen::Isometry3d, Eign::aligned_allocator\<Eigen::Isometry3d> > 

    > Isometry3d:欧式变换矩阵$(4\times4)$；poses的vector中含有默认参数aligned_allocator（继承了allocator）负责提供vector需要用到的动态内存（Eigen管理内存时需要单独强调元素的内存分配和管理）。

- 算法描述

  1. 打开文件，读入5张彩色图，5张深度图和对应位姿；若文件正确读取，输出图像及其大小。

  2. 定义相机内参数据$f_x, f_y, c_x, c_y,depthScale $

  3. 定义点云格式并新建一个点云

  4. 遍历5张图像，对一张图像的某个像素$[u,v]$

     1. 取对应深度值$d$（若$d=0$，表示没有测量到该点深度值，跳过）
     2. 计算$[u,v]$在相机坐标系下位置point
        - point[2]$\leftarrow$ d/depthScale，
        - point[0]$\leftarrow$(u-cx)*point[2]/fx
        - point[1]$\leftarrow$(v-cy)*point[2]/fy
     3. 计算世界坐标$p_w\leftarrow T*point$
     4. 根据得到的$p_w$和对应位置的彩色建立点并加入点云中

  5. 将所有点输出到map.pcd中。

       

- 部分代码解析

  1. 创建相机位姿的过程也是将位姿从四元数和平移向量转换为变化矩阵的过程（ch3）。直接用四元数对象$[q_w, q_x, q_y, q_z]$创建对应的变换矩阵（欧拉旋转矩阵）$T$*（记得四元数到旋转矩阵的转换吗）*，最后把$T$的平移向量设为位姿的平移向量$[x,y,z]$。

     ```c++
     double data[7] = {0};
     for ( auto& d:data )    
         fin>>d;
     Eigen::Quaterniond q( data[6], data[3], data[4], data[5] );
     Eigen::Isometry3d T(q);
     T.pretranslate( Eigen::Vector3d( data[0], data[1], data[2] ));
     poses.push_back( T );
     ```

  2. 相机坐标point通过**内参K**变换到像素坐标$[u,v]$
     $$
     d\begin{bmatrix}u\\v\\1\end{bmatrix} = Kp_c
     $$
     此处则为反推得到相机坐标point

     ```c++
     Eigen::Vector3d point; 
     point[2] = double(d)/depthScale; 
     point[0] = (u-cx)*point[2]/fx;
     point[1] = (v-cy)*point[2]/fy; 
     Eigen::Vector3d pointWorld = T*point;
     ```

     

     

  3. 相机坐标point通过**外参**变换到世界坐标point_world，记得ch3中得到的变换矩阵$T = \begin{bmatrix}R&t\\0^T&1 \end{bmatrix}$   (ch3)

     ```c++
     Eigen::Vector3d pointWorld = T*point;
     ```



**运行结果**



![ADX8RH.md.jpg](https://s2.ax1x.com/2019/03/31/ADX8RH.md.jpg)



**编译问题避雷**

说明：PCL是通过终端直接安装的

```
sudo apt-get install libpcl-dev pcl-tools
```

1. 报错：

   > No rule to make target '/usr/lib/x86_64-linux-gnu/libproj.so', needed by 'joinMap'

   解决：

   ```
    sudo ln -s  /usr/lib/x86_64-linux-gnu/libproj.so.9 /usr/lib/x86_64-linux-gnu/libproj.so
   ```

2. 再次make，报错：

   > /usr/bin/ld: cannot find -lvtkproj4

   解决：

   1. 安装依赖库

      ```
      sudo apt-get install libproj-dev
      ```

   2. 修改CMakeList.txt：在add_executable语句前加上

      ```
      list(REMOVE_ITEM PCL_LIBRARIES "vtkproj4")
      ```

      





### 2. Ceres库拟合曲线

设一个非线性模型
$$
y= exp(ax^2+bx+c)+w
$$
现有N个观测数据点，求曲线参数$a,b,c$，即
$$
min_{a,b,c}\frac{1}{2}\sum^{N}_{i=1}||y_i-exp(ax^2+bx+c)||^2
$$
**算法描述**

0. 生成真实值并在真实值中加入随机高斯噪声模拟观测值
1. 定义Cost Function类型用于计算残差
2. 通过代价函数构建构建最小二乘问题
3. 配置求解器参数并求解问题

**核心代码**

1. Cost 函数中需要定义'()'运算符函数，返回用输入参数abc计算的(_x,\_y)处的残差：

   ```c++
   template <typename T>
   bool operator() (const T* const abc, T* residual ) const     
   {
       residual[0] = T ( _y ) - ceres::exp ( abc[0]*T ( _x ) *T ( _x ) + abc[1]*T ( _x ) + abc[2] ); // y-exp(ax^2+bx+c)
       return true;
   }
   ```

2. 构建最小二乘问题：使用ceres库的Problem类的*AddResidualBlock*方法。

   - *AddResidualBlock* 负责将误差项添加到目标函数中。由于优化需要梯度，这里选择使用自动求导函数*AddDiffCostFunction*。后面两个参数分别为核函数（不使用）和待求解参数。
   - *AddDiffCostFunction\<T,i,j>*本身是一个模板函数，参数为：<误差类型，输出维度，输入维度>。这里1为输出一维残差值，3为输入参数abc，与前面定义的Cost functions中一致。

   ```c++
   ceres::Problem problem;
   problem.AddResidualBlock (new ceres::AutoDiffCostFunction<CURVE_FITTING_COST, 1, 3> (new CURVE_FITTING_COST ( x_data[i], y_data[i] )), nullptr,abc);
   ```

3. 配置求解器并求解问题（这里删掉了计时函数）

   ```c++
       // 配置求解器
       ceres::Solver::Options options;     // 配置项
       options.linear_solver_type = ceres::DENSE_QR;  // 定义增量方程求解方法
       options.minimizer_progress_to_stdout = true;   // 输出到cout
   
       ceres::Solver::Summary summary;                // 创建一个优化信息的summary
       ceres::Solve ( options, &problem, &summary );  // 开始优化
       cout<<summary.BriefReport() <<endl;            // 输出结果
   ```

**结果分析**

设定的参数真实值为$a =1, b = 2, c = 1$。

运行结果非常长，截取了一下信息，完整的放到了报告最后。

> solve time cost = 0.00353049 seconds. 
> Ceres Solver Report: Iterations: 22, Initial cost: 1.824887e+04, Final cost: 5.096854e+01, Termination: CONVERGENCE
> estimated a,b,c = 0.891943 2.17039 0.944142

可以看到，程序通过固定区间取x值并在相应y值处加入高斯噪声，生成了100个“观测点”。整体误差从1.824887e+04下降到5.096854e+01，梯度越来越小，并且在迭代22次后算法收敛。最后参数估计值如下，与真实值较为接近。
$$
a= 0.891943\ \  b= 2.17039\ \  c= 0.944142
$$




### 3. VO 0.1 0.2

**数据集说明**：

来自TUM提供的公开RGB-D数据集[fr1_xyz](http://vision.in.tum.de/data/datasets/rgbd-dataset)

1. rgb.txt和depth.txt记录各个文件的采集时间和对应文件名：

   ![depth-txt](C:\Users\Yuki\Desktop\待上传\depth-txt.JPG)

2. rgb/和depth/目录存档采集到的PNG格式图像文件。彩色图像为八位3通道，深度图像为16位单通道，文件名即采集时间。

3. groundtruth.txt为外部运动捕捉系统采集到的相机位姿，视为“标准轨迹”，格式为
   $$
   (time,t_x, t_y, t_z,q_x,q_y,q_z,q_w)
   $$

4. 在使用数据集之前需要用给出的Python脚本对根据时间进行对齐。结果（配对后的两幅图像的时间、文件名信息）放在associate.txt中

- rgb：*1305031102.175304.png*

![1305031102.175304](C:\Users\Yuki\Desktop\待上传\1305031102.175304.png)

- depth ：*1305031102.160407.png*

  ![1305031102.160407](C:\Users\Yuki\Desktop\待上传\1305031102.160407.png)

数据集使用：

1. 在config/default.yaml中填写数据集所在路径：

   ```
   /home/forslam/datasets/rgbd_dataset_freiburg1_xyz
   ```

2. 运行命令：

   ```
   bin/run.vo config/default.yaml
   ```

   



**0**.**2** **运行结果**：

1. 视频输出：左边显示当前帧图像，右边是响应的估计位置。

   ![run-vo](C:\Users\Yuki\Desktop\待上传\run-vo.JPG)

2. terminal输出

   > forslam@ubuntu:~/slambook/project/0.2$ bin/run_vo config/default.yaml 
   > dataset: /home/forslam/datasets/rgbd_dataset_freiburg1_xyz
   > read total 793 entries
   > Key frame size = 0
   > VO costs time: 0.021944
   > good matches: 253
   > pnp inliers: 251
   > VO costs time: 0.049344
   >
   > ....





## 5. 附录

### 主要的学习材料

- 半闲居士博客园](https://www.cnblogs.com/gaoxiang12/p/3695962.html)
- 《视觉SLAM十四讲》[BLBL公开课](https://www.bilibili.com/video/av19397094?from=search&seid=9220779439046733623)   [Github代码](https://github.com/gaoxiang12/slambook)
- ORB-SLAM论文及源码

### 部分源码

**从视频流获得用于相机标定的图像序列**

```python
import os
import skvideo.io
import numpy as np

video = skvideo.io.vreader('./videos/Video@2019_0606_140433.wmv', num_frames=30)

i = 0
for frame in video:
	skvideo.io.vwrite('./imgs/originframe{:0>3d}.png', frame)
	i = i+1

frame0 = skvideo.io.vread('./imgs/originframe000.png')
T, M, N, C = frame0.shape;

print(T)
print(M)
print(N)
print(C)
```



**相机标定** 输入图像序列 输出相机内参和畸变系数

```python
import numpy as np
import cv2
import glob
import os
 
# termination criteria
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.001)

row_no = 9
col_no = 7

#这里用的图7行*9列
# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
objp = np.zeros((row_no*col_no,3), np.float32)
objp[:,:2] = np.mgrid[0:row_no,0:col_no].T.reshape(-1,2)

# Arrays to store object points and image points from all the images.
objpoints = [] # 3d point in real world space
imgpoints = [] # 2d points in image plane.

images = os.listdir('./img/')

for fname in images:
    img = cv2.imread('./img/'+fname)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

    # Find the chess board corners
    ret, corners = cv2.findChessboardCorners(gray, (row_no,col_no),None)

    # If found, add object points, image points (after refining them)
    if ret == True:
        objpoints.append(objp)

        corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)
        imgpoints.append(corners2)

        # Draw and display the corners
        img = cv2.drawChessboardCorners(img, (row_no,col_no), corners2,ret)
        cv2.imshow('img',img)
        cv2.waitKey(30)

cv2.destroyAllWindows()

ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)
print(mtx)
print(dist)
np.savetxt('./matrix.txt',mtx)
np.savetxt('./dist.txt',dist)


h,  w = img.shape[:2]
newcameramtx, roi=cv2.getOptimalNewCameraMatrix(mtx,dist,(w,h),1,(w,h))
for fname in images:
    img = cv2.imread('./img/'+fname)
    # 通过调用函数，传递ROI参数就可以复制结果。
    dst = cv2.undistort(img, mtx, dist, None, newcameramtx)
    # crop the image
    x, y, w, h = roi
    dst = dst[y:y + h, x:x + w]
    cv2.imshow('img', img)
    cv2.imshow('dst', dst)
    cv2.waitKey(0)
    # 首先找到原图片与校正图片之间映射函数。然后使用重映射函数。
    # undistort
    # mapx, mapy = cv2.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w, h), 5)
    # dst = cv2.remap(img, mapx, mapy, cv2.INTER_LINEAR)
    #
    # # crop the image
    # x, y, w, h = roi
    # dst = dst[y:y + h, x:x + w]
    # cv2.imwrite('calibresult.png', dst)

```



**采集数据集** 输入视频 输出图像集和txt

```python
# coding:utf-8

import os
import cv2
import numpy as np
from multiprocessing import Pool

class Kitti():
    def __init__(self, video_parent_dir, frame_interval, img_width, img_height):
        '''
        :param video_father_dir:  video father dir file dir
        :param frame_interval:   every frame_interval save a key frame
        :param img_width:   img sequence width
        :param img_height:  img sequence height
        '''

        self.video_dirs = [video_parent_dir+dir for dir in os.listdir(video_parent_dir)]
        self.frame_interval = frame_interval
        self.img_width = img_width
        self.img_height = img_height
        self.dir_cnt = 0

    def get_format_name(self, idx, lenght):
        '''
        :param idx: given img index, such as 1, 2, 3
        :lenght: format name length
        :return: return format img name like 000001, 000002, ...
        '''
        cnt = lenght - 1
        prefix = ''
        nmb = idx
        while idx // 10 != 0:
            cnt -= 1
            idx = idx // 10
        for i in range(cnt):
            prefix += '0'
        return prefix + str(nmb)
    def run(self,video_dir):
        videoCapture = cv2.VideoCapture(video_dir)
        # get video fps
        fps = videoCapture.get(cv2.CAP_PROP_FPS)
        # get vide width and height
        size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH)), int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)))

        width_offset = int((size[0] - self.img_width) / 2)
        height_offset = int((size[1] - self.img_height) / 2)
        print('=======================')
        print('init_size: ',size)
        print('dest_size: (',self.img_width,',',self.img_height,')')
        img_sequence_parentdir = './' + self.get_format_name(self.dir_cnt, 2)
        if not os.path.exists(img_sequence_parentdir):
            os.makedirs(img_sequence_parentdir)
            if not os.path.exists(img_sequence_parentdir + '/image_0/'):
                os.makedirs(img_sequence_parentdir + '/image_0/')
        # 读帧
        success, frame = videoCapture.read()
        total_frame_idx = 0  # video frame index
        count = 0  # keyframe number
        tmp_cnt = 0  # record new cycle frame number
        timestep = 0  # time step
        timestep_total = [0]  # save every time step
        print("Fps: ", fps)
        print('=======================')
        while success:
            # 每秒采2帧
            if tmp_cnt == 0 or tmp_cnt == self.frame_interval:
                format_img_name = img_sequence_parentdir + '/image_0/' + self.get_format_name(count, 6) + '.png'
                print(format_img_name)
                cv2.imwrite(format_img_name,
                            frame[height_offset:height_offset + self.img_height,
                            width_offset:width_offset + self.img_width])
                count += 1
                tmp_cnt = 0
                timestep += self.frame_interval / fps
                timestep_total.append(timestep)

            success, frame = videoCapture.read()  # 获取下一帧
            total_frame_idx = total_frame_idx + 1
            tmp_cnt += 1
        self.dir_cnt += 1
        np.savetxt(img_sequence_parentdir + '/times.txt', timestep_total)

    def main(self):
        # pool = Pool()
        # pool.map(self.run,self.video_dirs)
        # pool.close()
        # pool.join()

        for video_dir in self.video_dirs:
            self.run(video_dir)


if __name__ == "__main__":
    K = Kitti('./videos/', 2, 640, 480)
    K.main()

```

### Ubuntu 16.下安装 OpenCV-3.2.0及opencv_contrib 3.2.0

*人生不想再经历一次了。*

命令

1. 安装依赖

   ```
   sudo apt-get install build-esscential
   ```

   

2. cmake.  如果你也是用来搞SLAM的话，后来要用到VTK库，所以最好还是一起装了吧省得后面还要重新编译所有源码。

```
$ unzip opencv-3.2.0.zip
$ mv opencv_contrib.zip ./opencv-3.2.0
$ cd opencv-3.2.0
$ unzip opencv_contrib
$
$ mkdir build 
$ cd build
$ cmake -D -D -D.. 
```

3. 编译（1小时+）

   ```
   make -j4
   ```

4. 安装

   ```
   sudo make install
   ```


走过20+小时弯路的tips

0. OpenCV和opencv_contrib一定都要从官网下载，且需要下载同一版本，能够避免99.999%的问题。[OpenCV官网](https://opencv.org/releases.html)&[opencv_contrib](https://github.com/opencv/opencv_contrib).Branch->tag中选择对应版本。

1. 保证分配给虚拟机足够的硬盘和内存大小（可以用 *dh -f* 命令查看），不然过程中容易出现“No enough space“。
2. cmake阶段卡在*ippicv_linux_20151201.zip*的下载中：ctrl+C退出进程，将自行下载的*ippicv_linux_20151201.zip*放进 /home/opencv-3.2.0/3dParty/ippicv/download/linux-xxxxxxx/下，重新开始cmake。
3. 在第三步可能遇到各种找不到 *.i文件、明明对应路径有cuda.hpp还是报错找不到等等问题，基本都可以通过第0步解决。（顺利到不真实感）