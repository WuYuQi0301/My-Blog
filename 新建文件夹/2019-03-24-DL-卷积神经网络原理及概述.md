---
title: DL 卷积神经网络原理及概述
date: 2019-03-24 07:30:00
categories:
- Deep Learning
tag:
- dl-notes

---



卷积神经网络



## CNN Basic

### 全连接神经网络

- 问题：
  - 不适合做图神经网络

### 卷积

本来想说图像处理中已经讲过很多次卷积了，但是发现并没有在博客中写过相关的总结，就这里叙述一下

- 卷积核（mask）
- 加上不同维度

  - 通道数
- 高维特征

  - 举例
- Feature learning：learn fiter to detect features

- 语义学习：更高维filter
- 高维度的filter
  - 更高维度的filter需要覆盖更大的图像区域，但是更高维度的特征经常onit物体的好的细节，所以不能仅仅是简单地扩大核size
  - 方法一：Stride：卷积步长
  - 方法2：Pooling：在每一个feature map上每2*2的方块中取最大值或者平均值。

- CNN

  ```mermaid
  graph LR
  A[Input]-->B[Con+ReLU]
  
  ```

  

  

## Model

### AlexNet

各项属性

1. ReLU
2. data augmentation
3. dropout p = 0.5
4. batch size = 128
5. ensemble = 7
6. SGD momentum 0.9
7. lr = 0.01，在一定时间内除以10
8. L2正则化（减少过拟合）

## VggNet

==pic==

16 layers

每一层的输入输出的大小和操作

第一层输入

64个channel

第二层输入

224*224\*224

数据存储：每一层的参数和输出

减少显存的方式

- :arrow_down:batch size
- :arrow_down:input data size
- :arrow_down:number of FC layers
- :arrow_down:number of input to FC layers
- :arrow_up: GPUs



如何减少第一层全连接层的输入数量？减少最后一层卷积层的输出的feature map的size，比如增加更多卷积层（通过pooling）

增加层数可能会使在训练集和测试集上的表现变差，在训练和测试上都增加了误差。

### ResNet

残差神经网络：使用网络层来学习residual mapping，而不是直接学习一个目标underlying mapping



为什么做的更好？

- 训练/更新网络每一层更容易
- 相当于一个由不同结构的CNN模型的ensemble

基本特点

- 





### DenseNets



将所有卷积层直接与其他卷积层相连，前$k-1$层卷积层的输出都直接导入第$k$层卷积层

- 与Resnet的差别：不是加和而是*concatenate* feature maps

- 好处：鼓励*feature reuse*，并能保证层级之间信息流最大化。层和层连接更多了，输出层的误差能更快传入输出层。也能利用更多的信息进行学习，卷积核的个数不需要套多



### Wide ResNet

增加学习层的宽度，结构不变（减少）的情况下层架卷积核的个数。

性能优于同层数的CNN；



### ResNeXt

==:crossed_swords:==

 将ResNet每个单元（block）拆分成小的变换再aggregate。

- cardinality 
- 每个block有相同个数的参数。
- 在复杂度较低的情况下性能优于 ResNet：training error更小，说明有更强的特征学习能力。

### SENet

==⚔️==

squeeze-and-excitation networks

- 寻找128维之中是否有相关维度的信息，并动态调整重要性



### PNASNet

- 消耗巨大的计算（GPU）资源



### Group Normalization

- 问题：Batch normalization 不适用于batch size很小的情况
- 思路：将通道（channel）分成组，在每个组内进行normalize
- LN(layer norm)和IN(instance norm)是特殊的GN，但他们不适用于视觉识别任务

## CNN应用

### Face recognition

- 往往需要有巨大的需要有ID的数据集

